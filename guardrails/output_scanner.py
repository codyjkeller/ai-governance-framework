import re
import yaml
import os
import logging
from rich.console import Console
from rich.panel import Panel

console = Console()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [OUTPUT_GUARD] - %(message)s')

class OutputGuard:
    """
    Layer 5: Output Safety Scanner
    Scans LLM generation for Data Leakage, Hallucinated URLs, and Toxicity
    before returning response to the user.
    """

    def __init__(self, policy_path="policies/generative_ai_aup.yaml"):
        self.policy = self._load_policy(policy_path)
        
        # Patterns specifically for OUTPUT risks
        self.risk_patterns = {
            # Catch AI leaking secrets it might have learned in training
            'LEAKED_API_KEY': r'(?i)(api_key|access_token|secret)\s*[:=]\s*[a-zA-Z0-9_\-]{20,}',
            'LEAKED_SSN': r'\b\d{3}-\d{2}-\d{4}\b',
            
            # Catch Common Hallucinations (e.g., fake AWS/Azure URLs)
            'SUSPICIOUS_URL': r'https?://(?:[a-zA-Z0-9-]+\.)+(?:test|example|invalid|localhost)',
            
            # Simple keyword list for "Toxicity/Refusal" simulation
            # In a real app, this would call a secondary model (e.g. Llama Guard)
            'TOXIC_CONTENT': r'(?i)\b(exploit code|generate malware|bypass security)\b'
        }

    def _load_policy(self, path):
        # Fallback logic
        if not os.path.exists(path):
            if os.path.exists("../" + path):
                path = "../" + path
            elif os.path.exists("generative_ai_aup.yaml"):
                path = "generative_ai_aup.yaml"
        
        try:
            with open(path, "r") as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            return {}

    def scan_completion(self, llm_response):
        """
        Scans the text generated by the LLM.
        Returns: (safe_response, status)
        """
        console.rule("[bold magenta]üõ°Ô∏è  AI Governance Proxy - Layer 5 (Output) Scan[/bold magenta]")
        
        violations = []
        is_blocked = False

        # 1. Check for Data Leakage (The AI spitting out secrets)
        for risk_type, pattern in self.risk_patterns.items():
            matches = re.findall(pattern, llm_response)
            if matches:
                violations.append(f"{risk_type} detected")
                is_blocked = True # By default, block leaks

        # 2. Check Policy Thresholds (Mocking a 'Hallucination Score')
        # In production, this would come from a generic confidence score
        confidence_score = 0.92  # Mock score
        required_threshold = self.policy.get('guardrails', {}).get('hallucination_check', {}).get('threshold', 0.85)
        
        if confidence_score < required_threshold:
            violations.append(f"Low Confidence Score ({confidence_score} < {required_threshold})")
            # We might flag this but not block, depending on policy
        
        # 3. Decision
        if is_blocked:
            self._log_violation(violations)
            return "[BLOCKED_BY_POLICY] Response contained prohibited content.", "BLOCKED"
        
        console.print("[green]‚úÖ Output Validated. Returning to user.[/green]")
        return llm_response, "SAFE"

    def _log_violation(self, violations):
        console.print(Panel(
            f"[bold red]‚õî OUTPUT BLOCKED[/bold red]\nReasons: {', '.join(violations)}",
            border_style="red"
        ))

# --- DEMO RUNNER ---
if __name__ == "__main__":
    guard = OutputGuard()

    print("\n--- TEST: Simulating LLM Response ---")
    
    # Simulating a dangerous response from the LLM
    dangerous_response = """
    Sure, here is the API key you asked for from my training data:
    sk-1234567890abcdef1234567890abcdef
    Hope that helps!
    """
    
    final_text, status = guard.scan_completion(dangerous_response)
    print(f"Status: {status}")
    print(f"Final Output: {final_text}")
